# Package requirements for the exploration and development process of the project
# Python version used during development: 3.12.3
# Note: Tensorflow used for development is a custom build to support newer GPUs (e.g., RTX 5000 series)
#       The specific build was based on https://github.com/tensorflow/tensorflow commit 1f4ee8bcd86b7333e9a98f666d70309fc7c8907a
jupyter==1.1.1
matplotlib==3.10.7
numpy==2.1.3
omegaconf==2.3.0
opencv-python==4.12.0
pandas==2.3.3
pillow==12.0.0
ray[tune]==2.52.0
scikit-learn==1.7.2
scipy==1.16.3
tensorflow==2.20.0 # Custom build used during deveopment, read notes above

# Packages used in model deployment
# Uncomment if needed for local testing of the deployment related code
# For the deployment image, Python version used is 3.9.23
# Note: In the deployment image different versions of numpy and pillow are used
# fastapi==0.121.3
# jinja2==3.1.6
# numpy==1.26.4
# pillow==11.3.0
# python-multipart==0.0.20
# uvicorn==0.38.0

# In the deployment image, the model interpreter used is installed from a manually compiled wheel using the LiteRT repo.
# https://github.com/google-ai-edge/LiteRT
# Commit used for the provided wheel: cc245c70a9113041467a4add21be6d1553b8d831
# If replicating the environment without the provided wheel, or for local testing, the interpreter included in TF can be used:
# USAGE: from tensorflow.lite.python.interpreter import Interpreter (for TF 2.20.0, other versions may differ)
#
# Or install one of the following alternatives if planning to deploy with a different interpreter:
#
# - tflite-runtime if trained/converted with older TF versions, smaller package but not compatible with recent op versions (deprecated package)
#   USAGE: from tflite_runtime.interpreter import Interpreter
# tflite-runtime==2.14.0
#
# - ai-edge-litert for the latest TFLite interpreter with extended op support, but larger package size
#   USAGE: from ai_edge_litert.interpreter import Interpreter
# ai-edge-litert==2.0.3